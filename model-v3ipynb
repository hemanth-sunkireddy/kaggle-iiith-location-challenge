{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOo036cG+5Dc51SReSA0Zy+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemanth-sunkireddy/kaggle-iiith-location-challenge/blob/main/model-v3ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQHY1KN-pe7K",
        "outputId": "c1432cc4-3c9e-4790-8c59-66e6ce3d32e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# Mount Google Drive\n",
        "# -------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# One-time copy (run before training)\n",
        "!cp -r /content/drive/MyDrive/SMAI_Project/images_train /content/\n",
        "!cp -r /content/drive/MyDrive/SMAI_Project/images_val /content/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/SMAI_Project/labels_train.csv /content/\n",
        "!cp -r /content/drive/MyDrive/SMAI_Project/labels_val.csv /content/"
      ],
      "metadata": {
        "id": "W8FBQzH3FHW3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision --force-reinstall"
      ],
      "metadata": {
        "id": "C8bW81CgrZHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"üöÄ Starting the enhanced image classification pipeline...\")\n",
        "\n",
        "# -------------------------\n",
        "# Set Paths\n",
        "# -------------------------\n",
        "base_path = '/content/drive/MyDrive/SMAI_Project'\n",
        "train_csv_path = os.path.join(base_path, 'labels_train.csv')\n",
        "val_csv_path = os.path.join(base_path, 'labels_val.csv')\n",
        "train_img_dir = '/content/images_train'\n",
        "val_img_dir = '/content/images_val'\n",
        "submission_output_path = os.path.join(base_path, '2022101005_1.csv')\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class RegionDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        print(f\"üì¶ Initialized dataset with {len(self.df)} samples from {img_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = row['Region_ID'] - 1  # 0-indexed\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# -------------------------\n",
        "# Transformations\n",
        "# -------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# Load Data\n",
        "# -------------------------\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "val_df = pd.read_csv(val_csv_path)\n",
        "\n",
        "train_dataset = RegionDataset(train_df, train_img_dir, train_transform)\n",
        "val_dataset = RegionDataset(val_df, val_img_dir, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -------------------------\n",
        "# Model & Optimizer\n",
        "# -------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üíª Using device: {device}\")\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Fine-tune entire model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, 15)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# -------------------------\n",
        "# Training Loop\n",
        "# -------------------------\n",
        "epochs = 15\n",
        "best_val_acc = 0\n",
        "\n",
        "print(\"üö¶ Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    pbar = tqdm(train_loader, desc=f\"üìö Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({'Loss': f\"{running_loss / (pbar.n + 1):.4f}\"})\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------\n",
        "    # Validation Accuracy\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            ground_truth.extend(labels.numpy())\n",
        "\n",
        "    val_acc = accuracy_score(ground_truth, predictions)\n",
        "    print(f\"‚úÖ Epoch {epoch+1} | Loss: {avg_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model (optional)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), os.path.join(base_path, 'best_model.pth'))\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "# -------------------------\n",
        "# Final Evaluation & Submission File\n",
        "# -------------------------\n",
        "print(\"üîç Final evaluation on validation set...\")\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "val_preds_df = pd.DataFrame({\n",
        "    'id': list(range(369)),\n",
        "    'Region_ID': [p + 1 for p in predictions]\n",
        "})\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    'id': list(range(369, 738)),\n",
        "    'Region_ID': [1] * 369\n",
        "})\n",
        "\n",
        "submission_df = pd.concat([val_preds_df, test_df], ignore_index=True)\n",
        "submission_df.to_csv(submission_output_path, index=False)\n",
        "print(f\"üìÅ Submission saved to {submission_output_path}\")\n",
        "\n",
        "print(\"üéâ All steps completed. Best Validation Accuracy: {:.4f}\".format(best_val_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "kq7Yv_1i_slm",
        "outputId": "25ce38d7-8120-49d8-fcaa-740388516f0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting the enhanced image classification pipeline...\n",
            "üì¶ Initialized dataset with 6542 samples from /content/images_train\n",
            "üì¶ Initialized dataset with 369 samples from /content/images_val\n",
            "üíª Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:01<00:00, 60.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üö¶ Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 1 | Loss: 1.4072 | Validation Accuracy: 0.7507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 2 | Loss: 0.5216 | Validation Accuracy: 0.8320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 3 | Loss: 0.2591 | Validation Accuracy: 0.8808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 4 | Loss: 0.1689 | Validation Accuracy: 0.8916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 5 | Loss: 0.1176 | Validation Accuracy: 0.9160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 6 | Loss: 0.0857 | Validation Accuracy: 0.9024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 7 | Loss: 0.0751 | Validation Accuracy: 0.8726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 8 | Loss: 0.0514 | Validation Accuracy: 0.9268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 9 | Loss: 0.0610 | Validation Accuracy: 0.9024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 10 | Loss: 0.0412 | Validation Accuracy: 0.9051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 11 | Loss: 0.0270 | Validation Accuracy: 0.9241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 12 | Loss: 0.0138 | Validation Accuracy: 0.9241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 13 | Loss: 0.0101 | Validation Accuracy: 0.9268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 14 | Loss: 0.0103 | Validation Accuracy: 0.9241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 15 | Loss: 0.0102 | Validation Accuracy: 0.9322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 16 | Loss: 0.0065 | Validation Accuracy: 0.9295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 17 | Loss: 0.0070 | Validation Accuracy: 0.9295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 18 | Loss: 0.0059 | Validation Accuracy: 0.9431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 19 | Loss: 0.0061 | Validation Accuracy: 0.9322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 20 | Loss: 0.0056 | Validation Accuracy: 0.9295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 21 | Loss: 0.0043 | Validation Accuracy: 0.9350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 22 | Loss: 0.0056 | Validation Accuracy: 0.9404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 23 | Loss: 0.0047 | Validation Accuracy: 0.9377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-14a456805982>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{running_loss / (pbar.n + 1):.4f}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"üöÄ Starting the enhanced image classification pipeline...\")\n",
        "\n",
        "# -------------------------\n",
        "# Set Paths\n",
        "# -------------------------\n",
        "base_path = '/content/drive/MyDrive/SMAI_Project'\n",
        "train_csv_path = os.path.join(base_path, 'labels_train.csv')\n",
        "val_csv_path = os.path.join(base_path, 'labels_val.csv')\n",
        "train_img_dir = '/content/images_train'\n",
        "val_img_dir = '/content/images_val'\n",
        "submission_output_path = os.path.join(base_path, '2022101005_1.csv')\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class RegionDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        print(f\"üì¶ Initialized dataset with {len(self.df)} samples from {img_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = row['Region_ID'] - 1  # 0-indexed\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# -------------------------\n",
        "# Transformations\n",
        "# -------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# Load Data\n",
        "# -------------------------\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "val_df = pd.read_csv(val_csv_path)\n",
        "\n",
        "train_dataset = RegionDataset(train_df, train_img_dir, train_transform)\n",
        "val_dataset = RegionDataset(val_df, val_img_dir, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -------------------------\n",
        "# Model & Optimizer\n",
        "# -------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üíª Using device: {device}\")\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Fine-tune entire model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, 15)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# -------------------------\n",
        "# Training Loop\n",
        "# -------------------------\n",
        "epochs = 100\n",
        "best_val_acc = 0\n",
        "best_model_state_dict = None\n",
        "\n",
        "print(\"üö¶ Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    pbar = tqdm(train_loader, desc=f\"üìö Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({'Loss': f\"{running_loss / (pbar.n + 1):.4f}\"})\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------\n",
        "    # Validation Accuracy\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            ground_truth.extend(labels.numpy())\n",
        "\n",
        "    val_acc = accuracy_score(ground_truth, predictions)\n",
        "    print(f\"‚úÖ Epoch {epoch+1} | Loss: {avg_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # If current validation accuracy is better, save the model and generate the CSV\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state_dict = model.state_dict()\n",
        "        torch.save(model.state_dict(), os.path.join(base_path, 'best_model.pth'))\n",
        "\n",
        "        # Generate the submission file\n",
        "        print(\"üîÑ Best model found, generating submission file...\")\n",
        "\n",
        "        # Generate predictions for submission\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        with torch.no_grad():\n",
        "            for images, _ in val_loader:\n",
        "                images = images.to(device)\n",
        "                outputs = model(images)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "        val_preds_df = pd.DataFrame({\n",
        "            'id': list(range(369)),\n",
        "            'Region_ID': [p + 1 for p in predictions]\n",
        "        })\n",
        "\n",
        "        test_df = pd.DataFrame({\n",
        "            'id': list(range(369, 738)),\n",
        "            'Region_ID': [1] * 369\n",
        "        })\n",
        "\n",
        "        submission_df = pd.concat([val_preds_df, test_df], ignore_index=True)\n",
        "        submission_df.to_csv(submission_output_path, index=False)\n",
        "        print(f\"üìÅ Submission saved to {submission_output_path}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "# -------------------------\n",
        "# Final Evaluation\n",
        "# -------------------------\n",
        "print(f\"üéâ Best Validation Accuracy: {best_val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SV4FmuxL1MSM",
        "outputId": "5a4e6825-8922-4b35-de0f-3acb61493a47"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting the enhanced image classification pipeline...\n",
            "üì¶ Initialized dataset with 6542 samples from /content/images_train\n",
            "üì¶ Initialized dataset with 369 samples from /content/images_val\n",
            "üíª Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üö¶ Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 1 | Loss: 1.4106 | Validation Accuracy: 0.7263\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 2 | Loss: 0.5282 | Validation Accuracy: 0.8401\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 3 | Loss: 0.2676 | Validation Accuracy: 0.8591\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 4 | Loss: 0.1605 | Validation Accuracy: 0.8916\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 5 | Loss: 0.1193 | Validation Accuracy: 0.8970\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 6 | Loss: 0.0734 | Validation Accuracy: 0.8916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 7 | Loss: 0.0593 | Validation Accuracy: 0.8916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 8 | Loss: 0.0899 | Validation Accuracy: 0.8889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 9 | Loss: 0.0717 | Validation Accuracy: 0.9024\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 10 | Loss: 0.0462 | Validation Accuracy: 0.9079\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 11 | Loss: 0.0197 | Validation Accuracy: 0.9187\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 12 | Loss: 0.0127 | Validation Accuracy: 0.9377\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 13 | Loss: 0.0109 | Validation Accuracy: 0.9431\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 14 | Loss: 0.0089 | Validation Accuracy: 0.9377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 15 | Loss: 0.0080 | Validation Accuracy: 0.9431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 16 | Loss: 0.0084 | Validation Accuracy: 0.9377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 17 | Loss: 0.0090 | Validation Accuracy: 0.9404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 18 | Loss: 0.0069 | Validation Accuracy: 0.9350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 19 | Loss: 0.0056 | Validation Accuracy: 0.9377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 20 | Loss: 0.0051 | Validation Accuracy: 0.9458\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 21 | Loss: 0.0058 | Validation Accuracy: 0.9350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 22 | Loss: 0.0063 | Validation Accuracy: 0.9377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 23 | Loss: 0.0056 | Validation Accuracy: 0.9295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7022b5ea3df1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{running_loss / (pbar.n + 1):.4f}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "class AngleDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        angle = row['angle']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, angle\n",
        "\n",
        "def mean_absolute_angular_error(true, pred):\n",
        "    true = np.array(true)\n",
        "    pred = np.array(pred)\n",
        "    return np.mean(np.minimum(np.abs(true - pred), 360 - np.abs(true - pred)))\n",
        "\n",
        "base_dir = '/content'\n",
        "train_csv = 'labels_train.csv'\n",
        "val_csv = 'labels_val.csv'\n",
        "train_img_dir = os.path.join(base_dir, 'images_train')\n",
        "val_img_dir = os.path.join(base_dir, 'images_val')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"‚úÖ Using device: {device}\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(base_dir, train_csv))\n",
        "val_df = pd.read_csv(os.path.join(base_dir, val_csv))\n",
        "\n",
        "train_dataset = AngleDataset(train_df, train_img_dir, transform)\n",
        "val_dataset = AngleDataset(val_df, val_img_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "best_maae = float('inf')\n",
        "submission_path = '2022101005_1.csv'\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, angles in tqdm(train_loader, desc=f\"üìö Epoch {epoch+1}/100\"):\n",
        "        images = images.to(device)\n",
        "        angles = angles.float().unsqueeze(1).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, angles)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"‚úÖ Epoch {epoch+1}: Loss = {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_gts = []\n",
        "    with torch.no_grad():\n",
        "        for images, angles in val_loader:\n",
        "            images = images.to(device)\n",
        "            angles = angles.numpy()\n",
        "            outputs = model(images).cpu().numpy().flatten()\n",
        "            val_preds.extend(outputs)\n",
        "            val_gts.extend(angles)\n",
        "\n",
        "    val_preds_clamped = [round(max(0, min(360, p))) for p in val_preds]\n",
        "    maae_score = mean_absolute_angular_error(val_gts, val_preds_clamped)\n",
        "    print(f\"üéØ Epoch {epoch+1}: Validation MAAE = {maae_score:.2f} degrees\")\n",
        "\n",
        "    if maae_score < best_maae:\n",
        "        best_maae = maae_score\n",
        "        val_submission = pd.DataFrame({\n",
        "            'id': list(range(len(val_preds_clamped))),\n",
        "            'angle': val_preds_clamped\n",
        "        })\n",
        "        dummy_test_submission = pd.DataFrame({\n",
        "            'id': list(range(len(val_preds_clamped), 738)),\n",
        "            'angle': [0] * (738 - len(val_preds_clamped))\n",
        "        })\n",
        "        final_submission = pd.concat([val_submission, dummy_test_submission], ignore_index=True)\n",
        "        final_submission.to_csv(submission_path, index=False)\n",
        "        print(f\"üíæ Best MAAE improved to {best_maae:.2f}, submission saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H7kPZWVEDoN8",
        "outputId": "48ee0868-6929-4570-e287-c46410d13de1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "üìö Epoch 1/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:38<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 1: Loss = 36796.2626\n",
            "üéØ Epoch 1: Validation MAAE = 90.08 degrees\n",
            "üíæ Best MAAE improved to 90.08, submission saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 2/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:39<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 2: Loss = 25714.7889\n",
            "üéØ Epoch 2: Validation MAAE = 93.15 degrees\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 3/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:39<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 3: Loss = 15455.1403\n",
            "üéØ Epoch 3: Validation MAAE = 73.17 degrees\n",
            "üíæ Best MAAE improved to 73.17, submission saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 4/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:39<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 4: Loss = 7992.1936\n",
            "üéØ Epoch 4: Validation MAAE = 69.95 degrees\n",
            "üíæ Best MAAE improved to 69.95, submission saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 5/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:39<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 5: Loss = 4560.8089\n",
            "üéØ Epoch 5: Validation MAAE = 65.23 degrees\n",
            "üíæ Best MAAE improved to 65.23, submission saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 6/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:39<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 6: Loss = 2864.3216\n",
            "üéØ Epoch 6: Validation MAAE = 62.99 degrees\n",
            "üíæ Best MAAE improved to 62.99, submission saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 7/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:39<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 7: Loss = 1887.6994\n",
            "üéØ Epoch 7: Validation MAAE = 62.58 degrees\n",
            "üíæ Best MAAE improved to 62.58, submission saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 8/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:39<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 8: Loss = 1213.5447\n",
            "üéØ Epoch 8: Validation MAAE = 61.31 degrees\n",
            "üíæ Best MAAE improved to 61.31, submission saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 9/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [01:39<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 9: Loss = 882.7534\n",
            "üéØ Epoch 9: Validation MAAE = 60.89 degrees\n",
            "üíæ Best MAAE improved to 60.89, submission saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 10/100:  31%|‚ñà‚ñà‚ñà       | 63/205 [00:30<01:08,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6238ba01bd5b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             )\n\u001b[0;32m--> 648\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    825\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚õ∞Ô∏è Colab and Environment Setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# üìÅ Define paths inside your Google Drive\n",
        "root_dir = '/content'\n",
        "train_csv = os.path.join(root_dir, 'labels_train.csv')\n",
        "val_csv = os.path.join(root_dir, 'labels_val.csv')\n",
        "train_img_dir = os.path.join(root_dir, 'images_train')\n",
        "val_img_dir = os.path.join(root_dir, 'images_val')\n",
        "\n",
        "# üßæ Read the CSVs\n",
        "train_df = pd.read_csv(train_csv)\n",
        "val_df = pd.read_csv(val_csv)\n",
        "\n",
        "# ‚ùå Remove anomaly IDs from val\n",
        "anomaly_ids = [95, 145, 146, 158, 159, 160, 161]\n",
        "val_df_cleaned = val_df[~val_df.index.isin(anomaly_ids)].reset_index(drop=True)\n",
        "\n",
        "# üñºÔ∏è Custom Dataset\n",
        "class GeoDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None, task='both'):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.task = task\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        lat = int(row['latitude'])\n",
        "        lon = int(row['longitude'])\n",
        "        return image, torch.tensor([lat, lon], dtype=torch.float)\n",
        "\n",
        "# üì¶ Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# üîÑ Dataloaders\n",
        "train_dataset = GeoDataset(train_df, train_img_dir, transform)\n",
        "val_dataset = GeoDataset(val_df_cleaned, val_img_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# üß† Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# ‚öôÔ∏è Loss & Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# üèãÔ∏è‚Äç‚ôÇÔ∏è Training\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"üìö Epoch {epoch+1} Loss: {running_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvPSERbgLUI8",
        "outputId": "e436672a-8a3f-4d2c-a8d4-c6ee1bd38a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 1 Loss: 318913120435.8244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:46<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 2 Loss: 318908278534.2439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 3 Loss: 318902027528.7415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:46<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 4 Loss: 318894862635.7073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:46<00:00,  4.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 5 Loss: 318881131135.3756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 6 Loss: 318880170953.0536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 7 Loss: 318865252661.6976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 8 Loss: 318856819497.2098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 9 Loss: 318853855341.8927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 10 Loss: 318844135863.5707\n",
            "üìù Generating submission...\n",
            "‚úÖ Submission file '2022101005_1.csv' created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ùå Remove anomaly IDs from val and preserve original indices\n",
        "anomaly_ids = [95, 145, 146, 158, 159, 160, 161]\n",
        "val_df_cleaned = val_df.drop(anomaly_ids).reset_index(drop=False)  # Keep original index as 'index'\n",
        "val_df_cleaned.rename(columns={'index': 'id'}, inplace=True)       # Rename index to 'id'\n",
        "\n",
        "# ...\n",
        "# üìà Validation\n",
        "model.eval()\n",
        "val_preds = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.round().cpu().numpy().astype(int)\n",
        "        val_preds.extend(preds)\n",
        "\n",
        "# üì§ Submission file\n",
        "print(\"üìù Generating submission...\")\n",
        "val_submission = pd.DataFrame({\n",
        "    'id': val_df_cleaned['id'].tolist(),  # Use original id\n",
        "    'Latitude': [lat for lat, lon in val_preds],\n",
        "    'Longitude': [lon for lat, lon in val_preds]\n",
        "})\n",
        "\n",
        "# Add 0,0 for test samples\n",
        "test_ids = list(range(369, 738))\n",
        "test_submission = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'Latitude': [0] * len(test_ids),\n",
        "    'Longitude': [0] * len(test_ids)\n",
        "})\n",
        "\n",
        "# Combine and save\n",
        "submission_df = pd.concat([val_submission, test_submission], ignore_index=True)\n",
        "submission_df = submission_df.sort_values(by='id').reset_index(drop=True)\n",
        "submission_df.to_csv('2022101005_1.csv', index=False)\n",
        "print(\"‚úÖ Submission file '2022101005_1.csv' created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-12S0q_O8eQ",
        "outputId": "9c601fe8-7ec5-4186-f81a-fb217edfa57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Generating submission...\n",
            "‚úÖ Submission file '2022101005_1.csv' created!\n"
          ]
        }
      ]
    }
  ]
}