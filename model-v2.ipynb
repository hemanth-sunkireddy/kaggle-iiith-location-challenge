{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMOR69qC4QO+AoRzhIwq2LD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemanth-sunkireddy/SMAI_Project/blob/main/SMAI_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQHY1KN-pe7K",
        "outputId": "c1432cc4-3c9e-4790-8c59-66e6ce3d32e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# Mount Google Drive\n",
        "# -------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# One-time copy (run before training)\n",
        "!cp -r /content/drive/MyDrive/SMAI_Project/images_train /content/\n",
        "!cp -r /content/drive/MyDrive/SMAI_Project/images_val /content/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/SMAI_Project/labels_train.csv /content/\n",
        "!cp -r /content/drive/MyDrive/SMAI_Project/labels_val.csv /content/"
      ],
      "metadata": {
        "id": "W8FBQzH3FHW3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision --force-reinstall"
      ],
      "metadata": {
        "id": "C8bW81CgrZHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"üöÄ Starting the enhanced image classification pipeline...\")\n",
        "\n",
        "# -------------------------\n",
        "# Set Paths\n",
        "# -------------------------\n",
        "base_path = '/content/drive/MyDrive/SMAI_Project'\n",
        "train_csv_path = os.path.join(base_path, 'labels_train.csv')\n",
        "val_csv_path = os.path.join(base_path, 'labels_val.csv')\n",
        "train_img_dir = '/content/images_train'\n",
        "val_img_dir = '/content/images_val'\n",
        "submission_output_path = os.path.join(base_path, '2022101005_1.csv')\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class RegionDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        print(f\"üì¶ Initialized dataset with {len(self.df)} samples from {img_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = row['Region_ID'] - 1  # 0-indexed\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# -------------------------\n",
        "# Transformations\n",
        "# -------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# Load Data\n",
        "# -------------------------\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "val_df = pd.read_csv(val_csv_path)\n",
        "\n",
        "train_dataset = RegionDataset(train_df, train_img_dir, train_transform)\n",
        "val_dataset = RegionDataset(val_df, val_img_dir, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -------------------------\n",
        "# Model & Optimizer\n",
        "# -------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üíª Using device: {device}\")\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Fine-tune entire model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, 15)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# -------------------------\n",
        "# Training Loop\n",
        "# -------------------------\n",
        "epochs = 15\n",
        "best_val_acc = 0\n",
        "\n",
        "print(\"üö¶ Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    pbar = tqdm(train_loader, desc=f\"üìö Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({'Loss': f\"{running_loss / (pbar.n + 1):.4f}\"})\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------\n",
        "    # Validation Accuracy\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            ground_truth.extend(labels.numpy())\n",
        "\n",
        "    val_acc = accuracy_score(ground_truth, predictions)\n",
        "    print(f\"‚úÖ Epoch {epoch+1} | Loss: {avg_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model (optional)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), os.path.join(base_path, 'best_model.pth'))\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "# -------------------------\n",
        "# Final Evaluation & Submission File\n",
        "# -------------------------\n",
        "print(\"üîç Final evaluation on validation set...\")\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "val_preds_df = pd.DataFrame({\n",
        "    'id': list(range(369)),\n",
        "    'Region_ID': [p + 1 for p in predictions]\n",
        "})\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    'id': list(range(369, 738)),\n",
        "    'Region_ID': [1] * 369\n",
        "})\n",
        "\n",
        "submission_df = pd.concat([val_preds_df, test_df], ignore_index=True)\n",
        "submission_df.to_csv(submission_output_path, index=False)\n",
        "print(f\"üìÅ Submission saved to {submission_output_path}\")\n",
        "\n",
        "print(\"üéâ All steps completed. Best Validation Accuracy: {:.4f}\".format(best_val_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "kq7Yv_1i_slm",
        "outputId": "25ce38d7-8120-49d8-fcaa-740388516f0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting the enhanced image classification pipeline...\n",
            "üì¶ Initialized dataset with 6542 samples from /content/images_train\n",
            "üì¶ Initialized dataset with 369 samples from /content/images_val\n",
            "üíª Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:01<00:00, 60.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üö¶ Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 1 | Loss: 1.4072 | Validation Accuracy: 0.7507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 2 | Loss: 0.5216 | Validation Accuracy: 0.8320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 3 | Loss: 0.2591 | Validation Accuracy: 0.8808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 4 | Loss: 0.1689 | Validation Accuracy: 0.8916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 5 | Loss: 0.1176 | Validation Accuracy: 0.9160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 6 | Loss: 0.0857 | Validation Accuracy: 0.9024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 7 | Loss: 0.0751 | Validation Accuracy: 0.8726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 8 | Loss: 0.0514 | Validation Accuracy: 0.9268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 9 | Loss: 0.0610 | Validation Accuracy: 0.9024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 10 | Loss: 0.0412 | Validation Accuracy: 0.9051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 11 | Loss: 0.0270 | Validation Accuracy: 0.9241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 12 | Loss: 0.0138 | Validation Accuracy: 0.9241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 13 | Loss: 0.0101 | Validation Accuracy: 0.9268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 14 | Loss: 0.0103 | Validation Accuracy: 0.9241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 15 | Loss: 0.0102 | Validation Accuracy: 0.9322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 16 | Loss: 0.0065 | Validation Accuracy: 0.9295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 17 | Loss: 0.0070 | Validation Accuracy: 0.9295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 18 | Loss: 0.0059 | Validation Accuracy: 0.9431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 19 | Loss: 0.0061 | Validation Accuracy: 0.9322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 20 | Loss: 0.0056 | Validation Accuracy: 0.9295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 21 | Loss: 0.0043 | Validation Accuracy: 0.9350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 22 | Loss: 0.0056 | Validation Accuracy: 0.9404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 23 | Loss: 0.0047 | Validation Accuracy: 0.9377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-14a456805982>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{running_loss / (pbar.n + 1):.4f}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"üöÄ Starting the enhanced image classification pipeline...\")\n",
        "\n",
        "# -------------------------\n",
        "# Set Paths\n",
        "# -------------------------\n",
        "base_path = '/content/drive/MyDrive/SMAI_Project'\n",
        "train_csv_path = os.path.join(base_path, 'labels_train.csv')\n",
        "val_csv_path = os.path.join(base_path, 'labels_val.csv')\n",
        "train_img_dir = '/content/images_train'\n",
        "val_img_dir = '/content/images_val'\n",
        "submission_output_path = os.path.join(base_path, '2022101005_1.csv')\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class RegionDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        print(f\"üì¶ Initialized dataset with {len(self.df)} samples from {img_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = row['Region_ID'] - 1  # 0-indexed\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# -------------------------\n",
        "# Transformations\n",
        "# -------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# Load Data\n",
        "# -------------------------\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "val_df = pd.read_csv(val_csv_path)\n",
        "\n",
        "train_dataset = RegionDataset(train_df, train_img_dir, train_transform)\n",
        "val_dataset = RegionDataset(val_df, val_img_dir, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -------------------------\n",
        "# Model & Optimizer\n",
        "# -------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üíª Using device: {device}\")\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Fine-tune entire model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, 15)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# -------------------------\n",
        "# Training Loop\n",
        "# -------------------------\n",
        "epochs = 100\n",
        "best_val_acc = 0\n",
        "best_model_state_dict = None\n",
        "\n",
        "print(\"üö¶ Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    pbar = tqdm(train_loader, desc=f\"üìö Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({'Loss': f\"{running_loss / (pbar.n + 1):.4f}\"})\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------\n",
        "    # Validation Accuracy\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            ground_truth.extend(labels.numpy())\n",
        "\n",
        "    val_acc = accuracy_score(ground_truth, predictions)\n",
        "    print(f\"‚úÖ Epoch {epoch+1} | Loss: {avg_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # If current validation accuracy is better, save the model and generate the CSV\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state_dict = model.state_dict()\n",
        "        torch.save(model.state_dict(), os.path.join(base_path, 'best_model.pth'))\n",
        "\n",
        "        # Generate the submission file\n",
        "        print(\"üîÑ Best model found, generating submission file...\")\n",
        "\n",
        "        # Generate predictions for submission\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        with torch.no_grad():\n",
        "            for images, _ in val_loader:\n",
        "                images = images.to(device)\n",
        "                outputs = model(images)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "        val_preds_df = pd.DataFrame({\n",
        "            'id': list(range(369)),\n",
        "            'Region_ID': [p + 1 for p in predictions]\n",
        "        })\n",
        "\n",
        "        test_df = pd.DataFrame({\n",
        "            'id': list(range(369, 738)),\n",
        "            'Region_ID': [1] * 369\n",
        "        })\n",
        "\n",
        "        submission_df = pd.concat([val_preds_df, test_df], ignore_index=True)\n",
        "        submission_df.to_csv(submission_output_path, index=False)\n",
        "        print(f\"üìÅ Submission saved to {submission_output_path}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "# -------------------------\n",
        "# Final Evaluation\n",
        "# -------------------------\n",
        "print(f\"üéâ Best Validation Accuracy: {best_val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV4FmuxL1MSM",
        "outputId": "5a4e6825-8922-4b35-de0f-3acb61493a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting the enhanced image classification pipeline...\n",
            "üì¶ Initialized dataset with 6542 samples from /content/images_train\n",
            "üì¶ Initialized dataset with 369 samples from /content/images_val\n",
            "üíª Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üö¶ Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 1 | Loss: 1.4106 | Validation Accuracy: 0.7263\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 2 | Loss: 0.5282 | Validation Accuracy: 0.8401\n",
            "üîÑ Best model found, generating submission file...\n",
            "üìÅ Submission saved to /content/drive/MyDrive/SMAI_Project/2022101005_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 3/100:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 82/103 [01:02<00:15,  1.34it/s, Loss=0.2756]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Dataset Class\n",
        "# -------------------------\n",
        "class AngleDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        angle = row['angle']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, angle\n",
        "\n",
        "# -------------------------\n",
        "# Angular Error Function\n",
        "# -------------------------\n",
        "def mean_absolute_angular_error(true, pred):\n",
        "    true = np.array(true)\n",
        "    pred = np.array(pred)\n",
        "    return np.mean(np.minimum(np.abs(true - pred), 360 - np.abs(true - pred)))\n",
        "\n",
        "# -------------------------\n",
        "# Setup\n",
        "# -------------------------\n",
        "base_dir = '/content'\n",
        "train_csv = 'labels_train.csv'\n",
        "val_csv = 'labels_val.csv'\n",
        "train_img_dir = os.path.join(base_dir, 'images_train')\n",
        "val_img_dir = os.path.join(base_dir, 'images_val')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"‚úÖ Using device: {device}\")\n",
        "\n",
        "# -------------------------\n",
        "# Load Data\n",
        "# -------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(base_dir, train_csv))\n",
        "val_df = pd.read_csv(os.path.join(base_dir, val_csv))\n",
        "\n",
        "train_dataset = AngleDataset(train_df, train_img_dir, transform)\n",
        "val_dataset = AngleDataset(val_df, val_img_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# -------------------------\n",
        "# Model\n",
        "# -------------------------\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 1)  # Output is a single angle\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# -------------------------\n",
        "# Training\n",
        "# -------------------------\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, angles in tqdm(train_loader, desc=f\"üìö Epoch {epoch+1}/{epochs}\"):\n",
        "        images = images.to(device)\n",
        "        angles = angles.float().unsqueeze(1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, angles)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"‚úÖ Epoch {epoch+1}: Loss = {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Validation\n",
        "# -------------------------\n",
        "model.eval()\n",
        "val_preds = []\n",
        "val_gts = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, angles in tqdm(val_loader, desc=\"üîç Evaluating\"):\n",
        "        images = images.to(device)\n",
        "        angles = angles.numpy()\n",
        "        outputs = model(images).cpu().numpy().flatten()\n",
        "        val_preds.extend(outputs)\n",
        "        val_gts.extend(angles)\n",
        "\n",
        "# -------------------------\n",
        "# Clamp and Calculate MAAE\n",
        "# -------------------------\n",
        "val_preds_clamped = [round(max(0, min(360, p))) for p in val_preds]\n",
        "maae_score = mean_absolute_angular_error(val_gts, val_preds_clamped)\n",
        "print(f\"üéØ Validation MAAE: {maae_score:.2f} degrees\")\n",
        "\n",
        "# -------------------------\n",
        "# Create Submission CSV\n",
        "# -------------------------\n",
        "val_submission = pd.DataFrame({\n",
        "    'id': list(range(len(val_preds_clamped))),\n",
        "    'angle': val_preds_clamped\n",
        "})\n",
        "\n",
        "dummy_test_submission = pd.DataFrame({\n",
        "    'id': list(range(len(val_preds_clamped), 738)),\n",
        "    'angle': [0] * (738 - len(val_preds_clamped))\n",
        "})\n",
        "\n",
        "final_submission = pd.concat([val_submission, dummy_test_submission], ignore_index=True)\n",
        "submission_path = os.path.join(base_dir, '2022101005_1.csv')\n",
        "final_submission.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"üìÅ Final submission saved: {submission_path}\")\n",
        "print(f\"üìä Final CSV shape: {final_submission.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7kPZWVEDoN8",
        "outputId": "f8305b83-4914-45de-d085-14695172b2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "üìö Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:46<00:00,  4.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 1: Loss = 39696.7334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 2: Loss = 34718.5034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:46<00:00,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 3: Loss = 29008.2694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 4: Loss = 22635.6866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 5: Loss = 16613.5200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 6: Loss = 11759.8191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 7: Loss = 8143.8862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 8: Loss = 5525.5154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:46<00:00,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 9: Loss = 3684.0104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìö Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 10: Loss = 2389.7512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üîç Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:01<00:00,  6.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Validation MAAE: 61.10 degrees\n",
            "üìÅ Final submission saved: /content/2022101005_1.csv\n",
            "üìä Final CSV shape: (738, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚õ∞Ô∏è Colab and Environment Setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# üìÅ Define paths inside your Google Drive\n",
        "root_dir = '/content'\n",
        "train_csv = os.path.join(root_dir, 'labels_train.csv')\n",
        "val_csv = os.path.join(root_dir, 'labels_val.csv')\n",
        "train_img_dir = os.path.join(root_dir, 'images_train')\n",
        "val_img_dir = os.path.join(root_dir, 'images_val')\n",
        "\n",
        "# üßæ Read the CSVs\n",
        "train_df = pd.read_csv(train_csv)\n",
        "val_df = pd.read_csv(val_csv)\n",
        "\n",
        "# ‚ùå Remove anomaly IDs from val\n",
        "anomaly_ids = [95, 145, 146, 158, 159, 160, 161]\n",
        "val_df_cleaned = val_df[~val_df.index.isin(anomaly_ids)].reset_index(drop=True)\n",
        "\n",
        "# üñºÔ∏è Custom Dataset\n",
        "class GeoDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None, task='both'):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.task = task\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        lat = int(row['latitude'])\n",
        "        lon = int(row['longitude'])\n",
        "        return image, torch.tensor([lat, lon], dtype=torch.float)\n",
        "\n",
        "# üì¶ Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# üîÑ Dataloaders\n",
        "train_dataset = GeoDataset(train_df, train_img_dir, transform)\n",
        "val_dataset = GeoDataset(val_df_cleaned, val_img_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# üß† Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# ‚öôÔ∏è Loss & Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# üèãÔ∏è‚Äç‚ôÇÔ∏è Training\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"üìö Epoch {epoch+1} Loss: {running_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvPSERbgLUI8",
        "outputId": "e436672a-8a3f-4d2c-a8d4-c6ee1bd38a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 1 Loss: 318913120435.8244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:46<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 2 Loss: 318908278534.2439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 3 Loss: 318902027528.7415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:46<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 4 Loss: 318894862635.7073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:46<00:00,  4.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 5 Loss: 318881131135.3756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 6 Loss: 318880170953.0536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 7 Loss: 318865252661.6976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 8 Loss: 318856819497.2098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 9 Loss: 318853855341.8927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:45<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Epoch 10 Loss: 318844135863.5707\n",
            "üìù Generating submission...\n",
            "‚úÖ Submission file '2022101005_1.csv' created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ùå Remove anomaly IDs from val and preserve original indices\n",
        "anomaly_ids = [95, 145, 146, 158, 159, 160, 161]\n",
        "val_df_cleaned = val_df.drop(anomaly_ids).reset_index(drop=False)  # Keep original index as 'index'\n",
        "val_df_cleaned.rename(columns={'index': 'id'}, inplace=True)       # Rename index to 'id'\n",
        "\n",
        "# ...\n",
        "# üìà Validation\n",
        "model.eval()\n",
        "val_preds = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.round().cpu().numpy().astype(int)\n",
        "        val_preds.extend(preds)\n",
        "\n",
        "# üì§ Submission file\n",
        "print(\"üìù Generating submission...\")\n",
        "val_submission = pd.DataFrame({\n",
        "    'id': val_df_cleaned['id'].tolist(),  # Use original id\n",
        "    'Latitude': [lat for lat, lon in val_preds],\n",
        "    'Longitude': [lon for lat, lon in val_preds]\n",
        "})\n",
        "\n",
        "# Add 0,0 for test samples\n",
        "test_ids = list(range(369, 738))\n",
        "test_submission = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'Latitude': [0] * len(test_ids),\n",
        "    'Longitude': [0] * len(test_ids)\n",
        "})\n",
        "\n",
        "# Combine and save\n",
        "submission_df = pd.concat([val_submission, test_submission], ignore_index=True)\n",
        "submission_df = submission_df.sort_values(by='id').reset_index(drop=True)\n",
        "submission_df.to_csv('2022101005_1.csv', index=False)\n",
        "print(\"‚úÖ Submission file '2022101005_1.csv' created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-12S0q_O8eQ",
        "outputId": "9c601fe8-7ec5-4186-f81a-fb217edfa57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Generating submission...\n",
            "‚úÖ Submission file '2022101005_1.csv' created!\n"
          ]
        }
      ]
    }
  ]
}